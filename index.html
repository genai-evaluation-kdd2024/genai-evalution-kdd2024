<!DOCTYPE html>
<html lang="en">
    <head>
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-S0FLVX7GFE"></script>
	<script>
  		window.dataLayer = window.dataLayer || [];
  		function gtag(){dataLayer.push(arguments);}
  		gtag('js', new Date());
  		gtag('config', 'G-S0FLVX7GFE');
	</script>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <title>GenAI Evaluation KDD 2024: workshop on Evaluation and Trustworthiness of Generative AI Models</title>
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.css" rel="stylesheet"  type='text/css'>
        <link href="css/styles.css" rel="stylesheet" />
    </head>

    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
            <div class="container px-4">
                <a class="navbar-brand" href="#" style="font-size:x-large;">
                    <img src="assets/amz_logo.png" height="30" alt="">
                    <!-- &nbsp;&nbsp;<span><i class="fa-brands fa-amazon"></i></span> -->
                </a>    
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="nav navbar-nav nav-pills ms-auto" style="font-size:medium; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif">
                        <li class="nav-item"><a class="nav-link" style="color:white;" href="#home" >Home</a></li>&nbsp;&nbsp;
                        <li class="nav-item"><a class="nav-link" style="color:white;" href="#keynotes" >Keynotes</a></li>&nbsp;&nbsp;
                        <li class="nav-item"><a class="nav-link" style="color:white;" href="#papers" >Accepted Papers</a></li>&nbsp;&nbsp;
                        <li class="nav-item"><a class="nav-link" style="color:white;" href="#organizers" >Organizer</a></li>&nbsp;&nbsp;
                        <li class="nav-item"><a class="nav-link" style="color:white;" href="#schedule" >Other Information</a></li>&nbsp;&nbsp;
                </ul>
                </div>
            </div>
        </nav>
        <!-- Header-->
        <header class="text-white" style="background:transparent url('assets/opener.jpeg') no-repeat center center /cover; opacity: 0.7;">
            <div class="container px-4 text-center" 
            style="background: rgba(0, 0, 0, 0.8); font-size:medium; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif">
                <h1 class="fw-bolder" style="color:ivory;">GenAI Evaluation KDD2024:</h1>
                <h2 class="fw-bolder">KDD workshop on Evaluation and Trustworthiness of Generative AI Models</h2>
                <p>Held in conjunction with <a style="color:#e77725;" href="https://kdd2024.kdd.org/">KDD'24</a></p>
            </div>
        </header>

        <!-- home section-->
        <section style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4" id="home">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <br>
                        <h2 style="text-align: center;">Welcome to GenAI Evaluation KDD 2024 !</h2>
                        <p style="text-align: justify;">
                            The landscape of machine learning and artificial intelligence has been profoundly reshaped by the advent of Generative AI Models and their applications, such as ChatGPT, GPT-4, Sora, and etc. Generative AI includes Large Language Models (LLMs) such as GPT, Claude, Flan-T5, Falcon, Llama, etc., and generative diffusion models. These models have not only showcased unprecedented capabilities but also catalyzed trans- formative shifts across numerous fields. Concurrently, there is a burgeoning interest in the comprehensive evaluation of Generative AI models, as evidenced by pioneering efforts in research bench- marks and frameworks for LLMs like PromptBench, BotChat, OpenCompass, MINT, and others. Despite these advancements, the quest to accurately assess the trustworthiness, safety, and ethical congruence of Generative AI Models continues to pose significant challenges. This underscores an urgent need for developing robust evaluation frameworks that can ensure these technologies are reliable and can be seamlessly integrated into society in a beneficial manner. Our workshop is dedicated to foster- ing interdisciplinary collaboration and innovation in this vital area, focusing on the development of new datasets, metrics, methods, and models that can advance our understanding and application of Generative AI.
                        </p>
                        <strong>Contact:</strong> <a href="mailto:kdd2024-ws-genai-eval@amazon.com">kdd2024-ws-genai-eval@amazon.com</a> 
                    </div>
                </div>
            </div>
        </section>

        <section class="bg-light" style="padding-top: 4%; padding-bottom: 4%;" hidden>
            <div class="container px-4" id="keynotes">
                <div class="row gx-4 justify-content-center">
                        <div class="col-lg-8">
                            <h2 style="text-align: center;">KEYNOTES</h2>
                            <h4 style="text-align: center;">Next-Generation Intelligent Assistants for AR/VR Devices
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="slides/keynote-1-slides.pdf" class="btn btn-secondary" role="button" target="_blank">Slides</a>
                                </div>  
                            </h4>

                        </div>
                </div>
                <br>
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-2">
                        <div>
                            <a class="navbar-brand js-scroll-trigger" href="https://lunadong.com/">
                            <span class="d-none d-lg-block"><img class="img-fluid img-profile mx-auto" src="./assets/luna.jpeg" alt="
                                Xin Luna Dong"></span>
                            </a>
                        </div>
                        <br>
                        <div>
                            <a style="color:#0275d8; text-decoration: none; display: inline-block; text-align: justify;" target="_blank" href="https://lunadong.com/"><b>Xin (Luna) Dong </b><br> Head Scientist <br> Meta</a>
                        </div>
                    </div>
                    <div class="col-lg-6" style="text-align: justify;">
                        <b>Abstract:</b> An intelligent assistant shall be an agent that knows you and the world, can receive your requests or predict your needs, and provide you the right services at the right time with your permission. As smart devices such as Amazon Alexa, Google Home, Meta Ray-ban Stories get popular, Intelligent Assistants are gradually playing an important role in people's lives. The Emergence of AR/VR devices brings more opportunities and calls for the next generation of Intelligent Assistants. In this talk, we discuss the many challenges and opportunities we face to grow intelligent assistants from voice-only to multi-modal, from context-agnostic to context-aware, from listening to the users' requests to predicting the user's needs, and from server-side to on-device. We expect these new challenges to open doors to new research areas and start a new chapter for providing personal assistance services.
                        <br><br>
                        <b>Bio:</b> Xin Luna Dong is a Principal Scientist at Meta Reality Labs, working on Intelligent Assistant. She has spent more than a decade building knowledge graphs, such as Amazon Product Graph and Google Knowledge Graph. She has co-authored books "Machine Knowledge: Creation and Curation of Comprehensive Knowledge Bases" and “Big Data Integration”. She was awarded VLDB Women in Database Research Award (2023) for "significant contributions to knowledge graph construction and data integration", ACM Distinguished Member (2018) for "significant contributions to data and knowledge integration", and VLDB Early Career Research Contribution Award (2016) for “advancing the state of the art of knowledge fusion”. She serves in the VLDB endowment and PVLDB advisory committee, and is a PC co-chair for KDD'2022 ADS track, WSDM'2022, VLDB'2021, and Sigmod'2018.
                    </div>
                </div>
                
                <br><br>

                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h4 style="text-align: center;">Graph-based Fusion for Multimodal Learning</h4>
                    </div>
                </div>
                <br>
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-6" style="text-align: justify;">
                        <b>Abstract:</b> With the advances in data collection techniques, large amounts of multimodal data collected from multiple sources are becoming available. Such multimodal data can provide complementary information that can reveal fundamental characteristics of real-world subjects. Thus, multimodal machine learning has become an active research area. Extensive works have been developed to exploit multimodal interactions and integrate multi-source information. In this talk, we will discuss using a graph-based multimodal fusion approach to enable multimodal fusion of incomplete data within a heterogeneous graph structure. This approach develops a unique strategy for learning on incomplete multimodal data without data deletion or data imputation. Moreover, we will discuss a dynamic graph-based approach to support federated training over multimodal distributed data without assuming similar active sensors in all clients. The key idea is to employ a dynamic and multi-view graph structure to adaptively capture the correlations amongst multimodal client models.                    
                        <br><br>
                        <b>Short Bio:</b> Dr. Aidong Zhang is Thomas M. Linville Endowed Professor of Computer Science, with joint appointments at Data Science, and Biomedical Engineering at University of Virginia (UVA). Prof. Zhang’s research interests include machine learning, data science, bioinformatics, and health informatics. Prof. Zhang is a fellow of ACM, IEEE, and AIMBE.
                    </div>
                    <div class="col-lg-2">
                        <a class="navbar-brand js-scroll-trigger" href="https://www.cs.virginia.edu/~az9eg/website/home.html">
                        <span class="d-none d-lg-block"><img class="img-fluid img-profile mx-auto" src="./assets/zhang.jpg" alt="Aidong Zhang"></span>
                        <p style="text-align: center;">
                            <a style="color:#0275d8; text-decoration: none; display: inline-block" target="_blank" href="https://www.cs.virginia.edu/~az9eg/website/home.html"><b>Aidong Zhang</b><br>Thomas M. Linville Professor of Computer Science<br>University of Virginia</a><br>
                        </p>                            
                        </a>
                    </div>
                </div>

            </div>

        </section>

        <section style="padding-top: 4%; padding-bottom: 4%; hidden hidden" hidden>
            <div class="container px-4" id="papers">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h2 style="text-align: center;">ACCEPTED PAPERS</h2>
                        <div class="list-group list-group-flush">
                            <a class="list-group-item list-group-item-action"></a>
                            <a class="list-group-item list-group-item-action">
                                <h5>Contrastive Multimodal Text Generation for E-Commerce Brand Advertising</h5>
                                <p>Nikhil Madaan, Krishna Kesari, Manisha Verma, Shaunak Mishra and Tor Steiner</p>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                <a href="papers/1-4 Contrastive Multimodal Text Generation for E-Commerce Brand Advertising.pdf" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div>
                            </a>   

                            <a class="list-group-item list-group-item-action">
                                <h5></h5>
                                <p></p>
                                <!--div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="slides/1-2-slides.pdf" class="btn btn-secondary" role="button" target="_blank">Slides</a>
                                    <a href="papers/1-2 Optimizing Learning Across Multimodal Transfer Features for Modeling Olfactory Perception.pdf" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div--> 
                            </a>   

                            <a class="list-group-item list-group-item-action">
                                <h5></h5>
                                <p>Jincheng Li, Chunyu Xie, Xiaoyu Wu, Bin Wang and Dawei Leng</p>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="" class="btn btn-secondary" role="button" target="_blank">Slides</a>
                                    <a href="" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div>                                
                            </a>   

                            <a class="list-group-item list-group-item-action">
                                <h5>Explainable Local and Global Models for Fine-Grained Multimodal Product Recognition</h5>
                                <p>Tobias Pettersson, Maria Riveiro and Tuwe Löfström</p>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="" class="btn btn-secondary" role="button" target="_blank">Slides</a>
                                    <a href="" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div>                             
                            </a>   

                            <a class="list-group-item list-group-item-action">
                                <h5>Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment</h5>
                                <p>Youxiang Zhu, Nana Lin, Xiaohui Liang, John Batsis, Robert Roth and Brian MacWhinney</p>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="" class="btn btn-secondary" role="button" target="_blank">Slides</a>
                                    <a href="" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div>                                
                            </a>   
 

                            <a class="list-group-item list-group-item-action">
                                <h5>Detecting text-rich objects: OCR or object detection? A case study with stopwatch detection</h5>
                                <p>Yarong Feng, Zongyi Liu, Yuan Ling, Shunyan Luo, Shujing Dong, Shuyi Wang and Bruce Ferry</p> 
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div>                            
                            </a>
                            
                            <a class="list-group-item list-group-item-action">
                                <h5>REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory</h5>
                                <p>Ziniu Hu</p>
                                <div class="btn-group btn-group-sm" role="group" aria-label="Basic example">
                                    <a href="" class="btn btn-secondary" role="button" target="_blank">Paper</a>
                                </div>                               
                            </a>  
                            <a class="list-group-item list-group-item-action"></a>
                        </div>             
                    </div>
                </div>
            </div>
        </section>

        <section class="bg-light" style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4"  id="organizers">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <br>
                        <h2 style="text-align: center;">Organizers - Amazon Teams</h2>
                        <br><br>
                        <div class="row">
                            <div class="mt-8 col-sm-3">
                                <a class="navbar-brand js-scroll-trigger" href="https://www.linkedin.com/in/yuan-ling-3572a75a/">
                                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto" src="./assets/ling.png" alt="Yuan Ling" ></span>
                                <p style="text-align: center;">
                                    <a style="color:#0275d8; text-decoration: none;" target="_blank" href="https://www.linkedin.com/in/yuan-ling-3572a75a/"><b>Yuan Ling</b></a><br>
                                </p>                            
                                </a>
                            </div>
                            <div class="mt-8 col-sm-1"> </div>
                            <div class="mt-8 col-sm-3">
                                <a class="navbar-brand js-scroll-trigger" href="https://www.linkedin.com/in/shujing-dong">
                                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto" src="./assets/dong.jpeg" alt="Shujing Dong"></span>
                                <p style="text-align: center;">
                                    <a style="color:#0275d8; text-decoration: none; display: inline-block" target="_blank" href="https://www.linkedin.com/in/shujing-dong"><b>Shujing Dong</b></a><br>
                                </p>                            
                                </a>
                            </div>
                            <div class="mt-8 col-sm-1"> </div>
                            <div class="mt-8 col-sm-3">
                                <a class="navbar-brand js-scroll-trigger" href="">
                                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto" src="./assets/joe.jpg" alt="Zongyi Liu"></span>
                                <p style="text-align: center;">
                                    <a style="color:#0275d8; text-decoration: none; display: inline-block" target="_blank" href=""><b>Zongyi Liu</b></a><br>
                                </p>
                                </a>
                            </div>
                        </div>
                        <div class="row">
                            <div class="mt-8 col-sm-3">
                                <a class="navbar-brand js-scroll-trigger" href="https://www.linkedin.com/in/yarong-feng-31bba172/">
                                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto" src="./assets/feng.png" alt="Yarong Feng"></span>
                                <p style="text-align: center;">
                                    <a style="color:#0275d8; text-decoration: none; display: inline-block" target="_blank" href="https://www.linkedin.com/in/yarong-feng-31bba172/"><b>Yarong Feng</b></a><br>
                                </p>                            
                                </a>
                            </div>
                            <div class="mt-8 col-sm-1"> </div>
                            <div class="mt-8 col-sm-3">
                                <a class="navbar-brand js-scroll-trigger" href="https://cse.umn.edu/cs/george-karypis">
                                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto" src="./assets/karypis.jpeg" alt="George Karypis" ></span>
                                <p style="text-align: center;">
                                    <a style="color:#0275d8; text-decoration: none; display: inline-block" target="_blank" href="https://cse.umn.edu/cs/george-karypis"><b>George Karypis</b></a><br>
                                </p>                            
                                </a>
                            </div>
                            <div class="mt-8 col-sm-1"> </div>
                            <div class="mt-8 col-sm-3">
                                <a class="navbar-brand js-scroll-trigger" href="https://people.cs.vt.edu/~reddy/">
                                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto" src="./assets/reddy.png" alt="Chandan K Reddy"></span>
                                <p style="text-align: center;">
                                    <a style="color:#0275d8; text-decoration: none; display: inline-block" target="_blank" href="https://people.cs.vt.edu/~reddy/"><b>Chandan K Reddy</b></a><br>
                                </p>                            
                                </a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section style="padding-top: 4%; padding-bottom: 4%;" hidden>
            <div class="container px-4" id="schedule">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-12">
                        <br>
                        <h2 style="text-align: center;">SCHEDULE</h2>
                        <p style="text-align: center;"> Sunday 25 August 2024 – Thursday 29 August 2024, Barcelona, Spain</p>
                            <div class="list-group list-group-flush">
                                <a class="list-group-item list-group-item-action">
                                  <div class="d-flex w-100 justify-content-between">
                                    <h5 class="mb-1"><span><i class="fas fa-book-open" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Opening</h5>
                                    <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;1:00 - 1:10 PM</h5>
                                  </div>
                                  <p class="mb-1">Introduction by organizers.</p>
                                </a>

                                
                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fa-solid fa-paperclip" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Keynote Talk 1: Next-Generation Intelligent Assistants for AR/VR Devices</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;1:10 - 1:50 PM</h5>
                                    </div>
                                    <p class="mb-1"> Xin (Luna) Dong <i style="color:grey">Principal Scientist, Meta</i></p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;1:50 - 2:05 PM</h5>
                                    </div>
                                    <p class="mb-1"> Youxiang Zhu, Nana Lin, Xiaohui Liang, John Batsis, Robert Roth and Brian MacWhinney</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Optimizing Learning Across Multimodal Transfer Features for Modeling Olfactory Perception</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;2:05 - 2:20 PM</h5>
                                    </div>
                                    <p class="mb-1">Daniel Shin, Gao Pei, Priyadarshini Kumari and Tarek Besold</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Explainable Local and Global Models for Fine-Grained Multimodal Product Recognition</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;2:20 - 2:35 PM</h5>
                                    </div>
                                    <p class="mb-1">Tobias Pettersson, Maria Riveiro and Tuwe Löfström</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Contrastive Multimodal Text Generation for E-Commerce Brand Advertising</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;2:35 - 2:50 PM</h5>
                                    </div>
                                    <p class="mb-1">Nikhil Madaan, Krishna Kesari, Manisha Verma, Shaunak Mishra and Tor Steiner</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;What Makes Good Open-Vocabulary Detector: A Disassembling Perspective</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;2:50 - 3:05PM</h5>
                                    </div>
                                    <p class="mb-1">Jincheng Li, Chunyu Xie, Xiaoyu Wu, Bin Wang and Dawei Leng</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fa-solid fa-mug-hot" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Coffee Break</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;3:05 - 3:30 PM</h5>
                                    </div>
                                </a>
                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fa-solid fa-paperclip" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Keynote Talk 2: Graph-based Fusion for Multimodal Learning</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;3:30 - 4:10 PM</h5>
                                    </div>
                                    <p class="mb-1"> Aidong Zhang <i style="color:grey">Thomas M. Linville Endowed Professor of Computer Science, University of Virginia</i></p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Detecting text-rich objects: OCR or object detection? A case study with stopwatch detection</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;4:10 - 4:25 PM</h5>
                                    </div>
                                    <p class="mb-1">Yarong Feng, Zongyi Liu, Yuan Ling, Shunyan Luo, Shujing Dong, Shuyi Wang and Bruce Ferry</p>
                                </a>

                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;4:25 - 4:40 PM</h5>
                                    </div>
                                    <p class="mb-1">Ziniu Hu</p>
                                </a>



                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-book" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Closing</h5>
                                      <h5><span><i class="fas fa-clock" style="color:#0275d8;"></i></span>&nbsp;&nbsp;4:55-5:00 PM</h5>
                                    </div>
                                    <p class="mb-1">Concluding remarks by organizers.</p>
                                </a>
                            </div>             
                </div>
            </div>
        </section>


        <!-- Submission Guidelines section-->
        <section class="bg-light" style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4" id="submission">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h2 style="text-align: center;">Call for Contributions</h2>
                        <p style="text-align: justify;">
                            This workshop aims to serve as a pivotal platform for discussing the forefront of Generative AI trustworthiness and evaluation advancements. Generative AI models, such as Large Language Models (LLMs) and Diffusion Models have revolutionized various domains, underscoring the critical need for reliable Generative AI technologies. As these models increasingly influence decision-making processes, establishing robust evaluation metrics and methods becomes paramount. Our objective is to delve into diverse evaluation strategies to enhance Generative AI models reliability across applications. The workshop topics include, but are not limited to:
                            <ul>
                                <li>Holistic Evaluation: Covering datasets, metrics, and methodologies</li>
                                <li>Trustworthiness in Generative AI Models:</li>
                                    <ul>
                                        <li>Truthfulness: Counteracting misinformation, hallucination, inconsistency, sycophancy in responses, adversarial factuality.</li>
                                        <li>Ensuring Safety and Security: privacy concerns, preventing harmful and toxicity content.</li>
                                        <li>Addressing Bias and Fairness.</li>
                                        <li>Ethical Considerations: social norm alignment, compliance with values, regulations and laws.</li>
                                        <li>Privacy: privacy awareness and privacy leakage. </li>
                                        <li>Enhancing misuse resistance, explainability, and robustness.</li>
                                    </ul>
                                <li>User-Centric Assessment.</li>
                                <li>Multi-perspective Evaluation: Emphasizing logical reasoning, knowledge depth, problem-solving, and user alignment.</li>
                                <li>Cross-Modal Evaluation: Integrating text, image, audio, etc. </li>
                             </ul>
                        </p>
                        <p style="text-align: justify;">
                            The workshop is designed to convene researchers from the realms of machine learning, data mining, and beyond, fostering the interdisciplinary exploration into Generative AI trustworthiness and evaluation. By featuring a blend of invited talks, presentations of peer-reviewed papers, and panel discussions, this workshop aims to facilitate exchanges of insights and foster collaborations across research and industry sectors. Participants from diverse fields such as Data Mining, Machine Learning, Natural Language Processing (NLP), and Information Retrieval are encouraged to share knowl- edge, debate challenges, and explore synergies, thereby advancing the state of the art in Generative AI technologies.
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <!-- Submission Dates section-->
        <section  style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4" id="dates">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <br>
                        <h2 style="text-align: center;">Important Dates</h2>

                        <ul>
                            <li>Link to the submission website: <a href="(TBD）">TBD</a></li>
                            <li>All deadlines are given in Anywhere on Earth time.</li>
                        </ul>

                            <div class="list-group list-group-flush">
                                <!-- <a class="list-group-item list-group-item-action">
                                  <div class="d-flex w-100 justify-content-between">
                                    <h5 class="mb-1"><span><i class="fas fa-book-open" style="color:#0275d8;"></i></span><s>&nbsp;&nbsp;Paper Submission Deadline</s></h5>
                                    <h5><span><i class="fas fa-calendar" style="color:#0275d8;"></i></span><s>&nbsp;&nbsp;June 9th, 2023</s></h5>
                                  </div>
                                </a> -->
                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-book-open" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Paper Submission Deadline</h5>
                                      <h5><span><i class="fas fa-calendar" style="color:#0275d8;"></i></span>&nbsp;&nbsp;May 28, 2024</h5>
                                    </div>
                                </a>
                                <a class="list-group-item list-group-item-action">
                                  <div class="d-flex w-100 justify-content-between">
                                    <h5 class="mb-1"><span><i class="fas fa-bars-progress" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Paper Acceptance Notification</h5>
                                    <h5><span><i class="fas fa-calendar" style="color:#0275d8;"></i></span>&nbsp;&nbsp;June 28, 2024</h5>
                                  </div>
                                </a>
                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-book" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Camera-Ready Submission</h5>
                                      <h5><span><i class="fas fa-calendar" style="color:#0275d8;"></i></span>&nbsp;&nbsp;August 12th, 2024</h5>
                                    </div>
                                </a>
                                <a class="list-group-item list-group-item-action">
                                    <div class="d-flex w-100 justify-content-between">
                                      <h5 class="mb-1"><span><i class="fas fa-circle-check" style="color:#0275d8;"></i></span>&nbsp;&nbsp;Workshop Date</h5>
                                      <h5><span><i class="fas fa-calendar" style="color:#0275d8;"></i></span>&nbsp;&nbsp;August 26th, 2024</h5>
                                    </div>
                                </a>
                            </div>             
                </div>
            </div>
        </section>        

        <section class="bg-light" style="padding-top: 4%; padding-bottom: 4%;">
            <div class="container px-4" id="guidelines">
                <div class="row gx-4 justify-content-center">
                    <div class="col-lg-8">
                        <h2 style="text-align: center;">Submission Guidelines</h2>
                        <p style="text-align: justify;">
                            <ul>
                                <li>Paper submissions are limited to 9 pages, excluding references, must be in PDF and use ACM Conference Proceeding templates (two column format). </li>
                                <li>Additional supplemental material focused on reproducibility can be provided. Proofs, pseudo-code, and code may also be included in the supplement, which has no explicit page limit. The supplement format could be either single column or double column. The paper should be self-contained, since reviewers are not required to read the supplement. </li>
                                <li>The Word template guideline can be found here: <a href="https://www.acm.org/publications/proceedings-template">[link]</a></li>
                                <li>The Latex/overleaf template guideline can be found here: <a href="https://www.overleaf.com/latex/templates/association-for-computing-machinery-acm-sig-proceedings-template/bmvfhcdnxfty">[link]</a></li></li>
                                <li>The submissions will be judged for quality and relevance through single-blind reviewing.</li>
                                
                                <li>A paper should be submitted in PDF format through EasyChair at the following link: <a href="https://easychair.org/my/conference?conf=multimodalkdd2023">[link]</a></li></li>
                            </ul> 
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Organizers section-->

        <!-- Footer-->
        <footer class="py-5 bg-dark">
            <div class="container px-4"><p class="m-0 text-center text-white">Copyright &copy; 2023</p></div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <script src="https://kit.fontawesome.com/f3573b1642.js" crossorigin="anonymous"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
